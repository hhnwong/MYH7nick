#top: check jobs

#extract mutation from excel
python ExtractExcelMutations.py

#remove duplicates in PD and SNP
sort -u file > newfile

#add accession code to the front of each line
# ^ is the start of a line
sed 's/^/P12883 /' snp.mut

#check there are no entries from both files
awk 'NR==FNR{a[$0];next} ($0 in a)' file1 file2

weka used: 3-8-3
##################################################
HCM/DCM set  with header

PD set without header

SNP without header
###################################################

02/10/19
extracted mutations from all datasets (Luis, DCM_HCM, Axiom) in *.mut 
duplicates removed in *_removed.mut

03/10/19
ran SAAP pipeline for all datasets output: ~/Documents/MYH7nick/*/*_out_date/JSON
converted JSON output files into csv output: ~/Documents/MYH7nick/*/*_out_date/CSV
concatenated all csv into one for each set *final.csv

04/10/19
concatenated three csv files from each set into one
snp_out_031019 wrong data, did not remove common mutations from DCM_HCM set
re-concatenated the data after modification to *final.csv
trained models (train_new) lower MCC than expected (-I (tree) 10 20 50 100 200 300 400) output: ~/Documents/MYH7nick/train_myh7/output/041019

07/10/19
corrected input csv in model training (removed duplicates in PD set from SAAP analysis)
new models (-I (tree) 10 20 50 100 200 300 400) output: ~/Documents/MYH7nick/train_myh7/output/071019_L456_K0_I400 with corrected input data
new models (-I (tree) 50 100 500 1000, -K 20) output: ~/Documents/MYH7nick/train_myh7/output/071019_L456_K20_I1000
new models with lower limit (300) to check if some 'SNPs' are actually SNP (-I (tree) 10 20 50 100 200 300 400) output: ~/Documents/MYH7nick/train_myh7/output/071019_L300_K0_I400
	compare with 071019_L456_K0_I400
	result: all models performing worse than using L456 except for one set, potential that some SNPs are not actually SNP
        future direction: explore number of features
09/10/19
new models (-I (tree) 50 100 500 1000, -K 0/10/15) output: ~/Documents/MYH7nick/train_myh7/output/091019_L456_K(0/10/15)_I1000
	benchmark features number, compare with 071019_L456_K20_I1000
	redo K0/10 I1000 training, no output in some models
	result: using -K 0/10 seems to obtain better results (-K 0 selected 32 features)
	future direction: rank features, engineer features

11/10/19
new models with built in features ranking (based on average impurity decrease, embedded method)

14/10/19
new models with features removed (all MLargest, NLargest, CisPro, SSGeom) Set 1
new models with features removed (6-10 MLargest, NLargest, CisPro, SSGeom) Set 2
new models with features removed (2-10 MLargest, NLargest, CisPro, SSGeom) Set 3
	result: 1 > 3 > 2 in terms of performance (MCC)
	future direction:
			-look at using the delta of the MLargestVoidX NLargestVoidX data rather than the numbers themselves
			-use pretrained predictor for our json files
				write some codes to extract the prediction and compare to see if it is actually PD or SNP, calculate confusion matrix
				use ~martin/scripts/mcc.pl -tp=xxx -fp=xxx -tn=xxx -fn=xxx to calculate MCC
			-clustering mutations, Plot the value from the left hand side of the dendogram against a linear x-axis to look for a sudden jump in values as an indicator of 			     the number of clusters and where an outlying set might be
			-have a look at using Kohonen self-organizing maps and PCA as ways of doing the clustering of the SNPs and looking for outliers

			-try support vector machines, PCA for features selection
			-try oversampling

			-Start to look at adding the other physiological data to the CSV files in order to start looking at sudden cardiac event prediction
			-Look online at methods for dealing with missing data and interpolated learning
				 Talk to Gil as one way of doing this is with auto-encoders

15/10/19
new models with engineered features (delta)
new models with features removed (Set1), overfitting (L912) (NOT DONE)
	result: delta set worked slightly better
		over-fitting not working, no models output

16/10/19
new models (delta) with -skip
	result: better result
new models (delta) with -skip and over-fitting
	result: MCC 0.9, might be misleading as it possibly takes the same data during 			training and testing (over-sampling)
	future direction: manual cross-validation outside weka

17/10/19
writing codes for independent cross-validation outside weka
	1. split sap and pd set to 10 and concatenate 8/9 into train*.csv and 1/9 into 		test.csv

18/10/19
independent cross-validation outside weka
	trial 1: set limit for both training and testing
	trial 2: only set limit for training, not testing
	result: all fail

20/10/19
independent cross-validation outside weka
	trail 3: repeat of trail 2
	result: fail

21/10/19
independent cross-validation outside weka
	identified issue, when combining pd and sip training file, csv doesn't have a line 	break at the end of the file
	trial 4: only set limit for training, not testing (fail)
	trial 5: set limit for both training and testing (fail)

trial 1-5: training and testing in separate loop
	trial 6: only set limit for training, not testing (fail)
		 training and testing within the same loop
	trial 7: only set limit for training, not testing
		 don't use -l tag, training and testing in the same line

29/10/19
split PD and SNP into 10 sets
	get list of mutations from pd.csv and snp.csv
	split mutations into 10, pull out all entry with those mutation into that set
	merge PD set 1 to SNP set 1

work on predicting sudden cardiac death
	clinical data, fill in missing data by averaging the rest first

01/11/19
list mutations that were mapped:
awk -F: '{print $4,$3,$5}' snp_final.csv | sort -u >> mapped_snp.mut
awk -F: '{print $4,$3,$5}' pd_all_final.csv | sort -u >> mapped_pd.mut

